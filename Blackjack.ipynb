{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMhro6wzu9Zj99MyUhWkGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LcLnAinIng/Rummikub-Simulation-with-Rreinforcement-Learning/blob/Blackjack/Blackjack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setting"
      ],
      "metadata": {
        "id": "ODWSebzU-e9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shx3syA29J3q"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FlattenObservation\n",
        "from gymnasium.utils.env_checker import check_env\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from typing import Optional\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "seed = 1310"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent - Q-Learning with Q-table"
      ],
      "metadata": {
        "id": "1GHbNxa_WJxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- State space is small → we can store Q(s,a) in a table (dict of arrays)\n"
      ],
      "metadata": {
        "id": "uNdxXoKOu3xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BlackjackAgent_QLearning_Qtable:\n",
        "    def __init__(self, env, episodes=1000, eps=0.1, alpha=0.1, gamma=0.9):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "        - env: the Blackjack environment\n",
        "        - episodes: number of training episodes\n",
        "        - eps: epsilon for epsilon-greedy (exploration rate)\n",
        "        - alpha: learning rate (step size for Q update)\n",
        "        - gamma: discount factor\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.episodes = episodes\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.action = [\"stick\", \"hit\"]\n",
        "\n",
        "        # Storage for Q-values (for Q-learning), can later swap to NN\n",
        "        self.Q = {}\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Decide which action to take given a state.\n",
        "        - With probability eps: pick random action (exploration)\n",
        "        - Otherwise: pick best action based on Q-values (exploitation)\n",
        "        Returns: action\n",
        "        \"\"\"\n",
        "        if state not in self.Q:\n",
        "          self.Q[state] = np.zeros(self.action_space.n)\n",
        "\n",
        "        if np.random.random() < self.eps:\n",
        "          action = np.random.choice(self.action_space.n)\n",
        "        else:\n",
        "          action = np.argmax(self.Q[state])\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_q(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Perform the Q-learning update rule:\n",
        "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
        "        \"\"\"\n",
        "        # Initialize unseen states\n",
        "        if state not in self.Q:\n",
        "          self.Q[state] = np.zeros(self.action_space.n)\n",
        "\n",
        "        if next_state not in self.Q:\n",
        "          self.Q[next_state] = np.zeros(self.action_space.n)\n",
        "\n",
        "        target = reward # Start with immediate reward\n",
        "\n",
        "        # Add future value if game not over\n",
        "        '''\n",
        "        if the episode is not finished, then we also care about the future reward we might get\n",
        "        `np.max(self.Q[next_state])` = best possible value I can get in the next state\n",
        "        gamma helps reduce importance of far-away rewards\n",
        "        add the future rewards to the immediate reward\n",
        "\n",
        "        `self.Q[state][action]` = Q(s,a)\n",
        "        This compare the current guess with the new target we have just computed\n",
        "        `(target - Q(s,a))` is the error in the estimate\n",
        "        multiply by learning rate `alpha` to take a small step toward fixing it\n",
        "        '''\n",
        "        if not done:\n",
        "          target += self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        self.Q[state][action] += self.alpha * (target - self.Q[state][action])\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Main training loop:\n",
        "        - For each episode:\n",
        "            1. Reset environment\n",
        "            2. Loop through steps until terminal state:\n",
        "                - Choose action\n",
        "                - Take action in env\n",
        "                - Get reward + next state\n",
        "                - Update Q-values\n",
        "        - Track performance (e.g., average reward)\n",
        "\n",
        "        updates Q-values, uses epsilon-greedy\n",
        "        \"\"\"\n",
        "\n",
        "        episode_rewards = []\n",
        "        total_reward = 0\n",
        "\n",
        "        for i in range(self.episodes):\n",
        "          state, info = self.env.reset()\n",
        "          done = False\n",
        "          while not done:\n",
        "            action = self.get_action(state)\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            self.update_q(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "          episode_rewards.append(total_reward)\n",
        "\n",
        "        return episode_rewards\n",
        "\n",
        "    def play(self, num_games=10):\n",
        "        \"\"\"\n",
        "        Run the agent in evaluation mode (greedy policy only).\n",
        "        Print/return results (e.g., wins, losses, draws).\n",
        "\n",
        "        no updates, greedy only.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        for _ in range(num_games):\n",
        "\n",
        "          state, info = self.env.reset()\n",
        "          done = False\n",
        "          total_reward = 0\n",
        "\n",
        "          while not done:\n",
        "            if state in self.Q:\n",
        "              action = np.argmax(self.Q[state])\n",
        "            else:\n",
        "              action = self.env.action_space.sample()\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "          results.append(total_reward)\n",
        "\n",
        "        # after all games, summarize\n",
        "        print(f'Average reward: {np.mean(results):.4f}')\n",
        "        print(f\"Wins: {results.count(1)}, Losses: {results.count(-1)}, Draws: {results.count(0)}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GbTydYceJc0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent - Q-Learning with DQN"
      ],
      "metadata": {
        "id": "fmxWetM-vKcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MiniDQNAgent:\n",
        "    def __init__(self, env, episodes=1000, eps=0.9, alpha=0.001, gamma=0.9):\n",
        "        \"\"\"\n",
        "        env: Blackjack environment\n",
        "        episodes: number of training episodes\n",
        "        eps: epsilon for epsilon-greedy\n",
        "        alpha: learning rate for optimizer\n",
        "        gamma: discount factor\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.episodes = episodes\n",
        "        self.eps = eps\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Neural network replaces Q-table\n",
        "        obs_size = 3  # (player_sum, dealer_card, usable_ace)\n",
        "        action_size = env.action_space.n\n",
        "        self.q_net = self.build_model(obs_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=alpha)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def build_model(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Small neural net: input = state, output = Q-values for each action.\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def encode_state(self, state):\n",
        "        \"\"\"\n",
        "        Convert environment state into a tensor for NN.\n",
        "        Example: (player_sum, dealer_card, usable_ace)\n",
        "        \"\"\"\n",
        "        return torch.tensor([state[0], state[1], int(state[2])], dtype=torch.float32)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Epsilon-greedy action selection.\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.eps:\n",
        "            return self.env.action_space.sample()  # random\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_net(self.encode_state(state))\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def update_q(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Perform one gradient descent step on TD error.\n",
        "        \"\"\"\n",
        "        # Convert to tensors\n",
        "        state_t = self.encode_state(state)\n",
        "        next_state_t = self.encode_state(next_state)\n",
        "\n",
        "        # Current Q(s,a)\n",
        "        q_values = self.q_net(state_t)\n",
        "        q_value = q_values[action]\n",
        "\n",
        "        # Target: r + gamma * max Q(s',a')\n",
        "        with torch.no_grad():\n",
        "            next_q = self.q_net(next_state_t)\n",
        "            max_next_q = torch.max(next_q)\n",
        "            target = reward + (0 if done else self.gamma * max_next_q)\n",
        "\n",
        "        # Loss = (target - Q(s,a))^2\n",
        "        loss = self.loss_fn(q_value, target)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Main training loop.\n",
        "        \"\"\"\n",
        "        for i in range(self.episodes):\n",
        "            state, info = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.get_action(state)\n",
        "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                self.update_q(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "\n",
        "    def play(self, num_games=10):\n",
        "        \"\"\"\n",
        "        Evaluate trained policy (greedy only).\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for _ in range(num_games):\n",
        "            state, info = self.env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            while not done:\n",
        "                with torch.no_grad():\n",
        "                    q_values = self.q_net(self.encode_state(state))\n",
        "                action = torch.argmax(q_values).item()\n",
        "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "            results.append(total_reward)\n",
        "\n",
        "        print(f\"Average reward: {np.mean(results):.2f}\")\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "50yEs9fevNi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the code"
      ],
      "metadata": {
        "id": "AvbFJcHNtP1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Create Blackjack environment\n",
        "    env = gym.make(\"Blackjack-v1\", natural = True, sab = False)  # sab=False = default rules\n",
        "\n",
        "    # Create agent\n",
        "    agent = BlackjackAgent_QLearning_Qtable(env, episodes=50000, eps=0.1, alpha=0.1, gamma=0.9)\n",
        "\n",
        "    # Train the agent\n",
        "    rewards = agent.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    # Evaluate performance\n",
        "    agent.play(num_games=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpVwrDj_tSDq",
        "outputId": "b841a077-c6cb-466d-eea4-ba9b4d9b64a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n",
            "Average reward: -0.1600\n",
            "Wins: 20, Losses: 28, Draws: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intuition with Blackjack example\n",
        "Suppose:\n",
        "\n",
        "You’re in state (sum=15, dealer=10, usable_ace=False).\n",
        "\n",
        "You choose action hit.\n",
        "\n",
        "You get reward 0 (game continues).\n",
        "\n",
        "Next state is (sum=18, dealer=10, usable_ace=False).\n",
        "\n",
        "Your table says Q(next_state) = [stick: 0.5, hit: -0.3].\n",
        "\n",
        "Then:\n",
        "\n",
        "reward = 0\n",
        "\n",
        "np.max(Q[next_state]) = 0.5\n",
        "\n",
        "target = 0 + gamma * 0.5 = 0.45 (if gamma=0.9)\n",
        "\n",
        "Suppose current Q(state, hit) = 0.1.\n",
        "\n",
        "Update:\n",
        "\n",
        "Q(15,10,hit)←0.1+0.9(0.45−0.1)=0.415\n",
        "\n",
        "So now the agent thinks hitting on 15 vs dealer 10 is worth about 0.415 expected value."
      ],
      "metadata": {
        "id": "XHAM395SiXnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VS Code Helper"
      ],
      "metadata": {
        "id": "3j-eLTRAgwNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lfdYzq0dtY8",
        "outputId": "4d9854af-39a6-49a8-a386-7d510cf6d42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirement.txt"
      ],
      "metadata": {
        "id": "fhgwaUXUdvGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}